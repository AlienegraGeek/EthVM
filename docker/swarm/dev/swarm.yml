version: '3.6'

networks:

  traefik:
    external: true

  backend:
    driver: overlay

volumes:

  traefik-volume:
    external: true

  zookeeper-1-volume:
    external: true
  zookeeper-2-volume:
    external: true
  zookeeper-3-volume:
    external: true

  kafka-1-volume:
    external: true
  kafka-2-volume:
    external: true
  kafka-3-volume:
    external: true

  kafka-streams-volume:
    external: true

  kafka-connect-volume:
    external: true

  mongodb-volume:
    external: true

  parity-volume:
    external: true

services:

  # ///////////////////////////////////////////////////////
  # Front Services
  # ///////////////////////////////////////////////////////

  traefik:
    image: enkryptio/traefik:0.1.3
    networks:
      - backend
      - traefik
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - traefik-volume:/var/lib/traefik
    ports:
      - 80
      - 443
      - 8080
    environment:
      DEBUG: "true"
      LOG_LEVEL: INFO
      SWARM_MODE: "true"
      LETS_ENCRYPT_ENABLED:
    deploy:
      # resources:
      #   limits:
      #     cpus: '3.0'
      #     memory: 6G
      #   reservations:
      #     cpus: '2.0'
      #     memory: 4G
      placement:
        constraints:
          - node.role == worker
      replicas: 1
      restart_policy:
        condition: on-failure

  explorer:
    image: enkryptio/explorer:0.1.0-development
    networks:
      - backend
    deploy:
      # resources:
      #   limits:
      #     cpus: '2.0'
      #     memory: 2G
      #   reservations:
      #     cpus: '1.0'
      #     memory: 1G
      placement:
        constraints:
          - node.role == worker
      replicas: 1
      restart_policy:
        condition: on-failure
      labels:
        - 'traefik.enable=true'
        - "traefik.docker.network=traefik"
        - 'traefik.default.protocol=http'
        - 'traefik.frontend.rule=Host:ropsten.ethvm.com'
        - 'traefik.frontend.headers.customResponseHeaders=Access-Control-Allow-Origin:*||Access-Control-Allow-Credentials:true'
        - 'traefik.frontend.passHostHeader=true'
        - 'traefik.backend=explorer'
        - 'traefik.port=80'

  api:
    image: enkryptio/api:0.1.0
    depends_on:
      - mongodb
    networks:
      - backend
    environment:
      ETHVM_MONGO_DB_URL: "mongodb://mongodb:27017/ethvm_dev?w=1&journal=true&maxIdleTimeMS=60000"
      ETHVM_MONGO_DB_NAME: ethvm_dev
    deploy:
      # resources:
      #   limits:
      #     cpus: '3.0'
      #     memory: 6G
      #   reservations:
      #     cpus: '2.0'
      #     memory: 4G
      placement:
        constraints:
          - node.role == worker
      replicas: 1
      restart_policy:
        condition: on-failure
      labels:
        - "traefik.enable=true"
        - "traefik.docker.network=traefik"
        - "traefik.default.protocol=http"
        - "traefik.frontend.rule=Host:api.ropsten.ethvm.com"
        - "traefik.frontend.headers.customResponseHeaders=Access-Control-Allow-Origin:https://ropsten.ethvm.com||Access-Control-Allow-Credentials:true"
        - "traefik.frontend.passHostHeader=true"
        - "traefik.backend=api"
        - "traefik.port=3000"

  # ///////////////////////////////////////////////////////
  # Storage
  # ///////////////////////////////////////////////////////

  mongodb:
    image: enkryptio/mongodb:4.0.5.2
    volumes:
      - mongodb-volume:/data/db
    networks:
      - backend
    environment:
      MONGO_WIRED_TIGER_CACHE_SIZE: 6
    deploy:
      # resources:
      #   limits:
      #     cpus: '5.0'
      #     memory: 10G
      #   reservations:
      #     cpus: '4.0'
      #     memory: 8G
      placement:
        constraints:
          - node.role == worker
          - node.labels.io.ethvm.nodeType == store
      replicas: 1
      restart_policy:
        condition: on-failure

  # ///////////////////////////////////////////////////////
  # Processing
  # ///////////////////////////////////////////////////////

  zookeeper-1:
    image: confluentinc/cp-zookeeper:5.0.1
    volumes:
      - zookeeper-1-volume:/var/lib/zookeeper
    networks:
      - backend
    environment:
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_SERVERS: "server.1=zookeeper-1:2888:3888 server.2=zookeeper-2:2888:3888 server.3=zookeeper-3:2888:3888"
      ZOOKEEPER_CLIENT_PORT: 2181
    deploy:
      # resources:
      #   limits:
      #     cpus: '2.0'
      #     memory: 4G
      #   reservations:
      #     cpus: '1.0'
      #     memory: 2G
      placement:
        constraints:
          - node.role == worker
          - node.labels.io.ethvm.nodeType == processing-worker-1
      replicas: 1
      restart_policy:
        condition: on-failure

  zookeeper-2:
    image: confluentinc/cp-zookeeper:5.0.1
    volumes:
      - zookeeper-2-volume:/var/lib/zookeeper
    networks:
      - backend
    environment:
      ZOOKEEPER_SERVER_ID: 2
      ZOOKEEPER_SERVERS: "server.1=zookeeper-1:2888:3888 server.2=zookeeper-2:2888:3888 server.3=zookeeper-3:2888:3888"
      ZOOKEEPER_CLIENT_PORT: 2181
    deploy:
      # resources:
      #   limits:
      #     cpus: '2.0'
      #     memory: 4G
      #   reservations:
      #     cpus: '1.0'
      #     memory: 2G
      placement:
        constraints:
          - node.role == worker
          - node.labels.io.ethvm.nodeType == processing-worker-2
      replicas: 1
      restart_policy:
        condition: on-failure

  zookeeper-3:
    image: confluentinc/cp-zookeeper:5.0.1
    volumes:
      - zookeeper-3-volume:/var/lib/zookeeper
    networks:
      - backend
    environment:
      ZOOKEEPER_SERVER_ID: 3
      ZOOKEEPER_SERVERS: "server.1=zookeeper-1:2888:3888 server.2=zookeeper-2:2888:3888 server.3=zookeeper-3:2888:3888"
      ZOOKEEPER_CLIENT_PORT: 2181
    deploy:
      # resources:
      #   limits:
      #     cpus: '2.0'
      #     memory: 4G
      #   reservations:
      #     cpus: '1.0'
      #     memory: 2G
      placement:
        constraints:
          - node.role == worker
          - node.labels.io.ethvm.nodeType == processing-worker-3
      replicas: 1
      restart_policy:
        condition: on-failure

  kafka-1:
    image: confluentinc/cp-kafka:5.0.1
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    networks:
      - backend
    ports:
      - target: 9094
        published: 9094
        protocol: tcp
        mode: host
    volumes:
      - kafka-1-volume:/var/lib/kafka
    environment:
      HOSTNAME_COMMAND: "docker info | grep ^Name: | cut -d' ' -f 2"
      KAFKA_BROKER_ID: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: INSIDE://:9092,OUTSIDE://_{HOSTNAME_COMMAND}:9094
      KAFKA_LISTENERS: INSIDE://:9092,OUTSIDE://:9094
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      CONFLUENT_SUPPORT_METRICS_ENABLE: "true"
      KAFKA_MESSAGE_MAX_BYTES: 52428800
      KAFKA_REPLICA_FETCH_MAX_BYTES: 52428800
      KAFKA_PRODUCER_MAX_REQUEST_SIZE: 5048576
      KAFKA_CONSUMER_MAX_PARTITION_FETCH_BYTES: 52428800
      KAFKA_JMX_PORT: 9586
    deploy:
      # resources:
      #   limits:
      #     cpus: '4.0'
      #     memory: 8G
      #   reservations:
      #     cpus: '3.0'
      #     memory: 4G
      placement:
        constraints:
          - node.role == worker
          - node.labels.io.ethvm.nodeType == processing-worker-1
      replicas: 1
      restart_policy:
        condition: on-failure

  kafka-2:
    image: confluentinc/cp-kafka:5.0.1
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    networks:
      - backend
    volumes:
      - kafka-2-volume:/var/lib/kafka
    environment:
      HOSTNAME_COMMAND: "docker info | grep ^Name: | cut -d' ' -f 2"
      KAFKA_BROKER_ID: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: INSIDE://:9092,OUTSIDE://_{HOSTNAME_COMMAND}:9094
      KAFKA_LISTENERS: INSIDE://:9092,OUTSIDE://:9094
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      CONFLUENT_SUPPORT_METRICS_ENABLE: "true"
      KAFKA_MESSAGE_MAX_BYTES: 52428800
      KAFKA_REPLICA_FETCH_MAX_BYTES: 52428800
      KAFKA_PRODUCER_MAX_REQUEST_SIZE: 5048576
      KAFKA_CONSUMER_MAX_PARTITION_FETCH_BYTES: 52428800
      KAFKA_JMX_PORT: 9586
    deploy:
      # resources:
      #   limits:
      #     cpus: '4.0'
      #     memory: 8G
      #   reservations:
      #     cpus: '3.0'
      #     memory: 4G
      placement:
        constraints:
          - node.role == worker
          - node.labels.io.ethvm.nodeType == processing-worker-2
      replicas: 1
      restart_policy:
        condition: on-failure

  kafka-3:
    image: confluentinc/cp-kafka:5.0.1
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
    networks:
      - backend
    volumes:
      - kafka-3-volume:/var/lib/kafka
    environment:
      HOSTNAME_COMMAND: "docker info | grep ^Name: | cut -d' ' -f 2"
      KAFKA_BROKER_ID: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: INSIDE://:9092,OUTSIDE://_{HOSTNAME_COMMAND}:9094
      KAFKA_LISTENERS: INSIDE://:9092,OUTSIDE://:9094
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
      CONFLUENT_SUPPORT_METRICS_ENABLE: "true"
      KAFKA_MESSAGE_MAX_BYTES: 52428800
      KAFKA_REPLICA_FETCH_MAX_BYTES: 52428800
      KAFKA_PRODUCER_MAX_REQUEST_SIZE: 5048576
      KAFKA_CONSUMER_MAX_PARTITION_FETCH_BYTES: 52428800
      KAFKA_JMX_PORT: 9586
    deploy:
      # resources:
      #   limits:
      #     cpus: '4.0'
      #     memory: 8G
      #   reservations:
      #     cpus: '3.0'
      #     memory: 4G
      placement:
        constraints:
          - node.role == worker
          - node.labels.io.ethvm.nodeType == processing-worker-3
      replicas: 1
      restart_policy:
        condition: on-failure

  kafka-schema-registry:
    image: confluentinc/cp-schema-registry:5.0.1
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
      - kafka-1
      - kafka-2
      - kafka-3
    networks:
      - backend
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: "PLAINTEXT://kafka-1:9091"
      SCHEMA_REGISTRY_HOST_NAME: kafka-schema-registry
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      SCHEMA_REGISTRY_LOG4J_ROOT_LOGLEVEL: "INFO"
      SCHEMA_REGISTRY_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"
    deploy:
      # resources:
      #   limits:
      #     cpus: '2.0'
      #     memory: 1G
      #   reservations:
      #     cpus: '1.0'
      #     memory: 512M
      placement:
        constraints:
          - node.role == worker
          - node.labels.io.ethvm.nodeType != processing-worker-1
          - node.labels.io.ethvm.nodeType != processing-worker-2
          - node.labels.io.ethvm.nodeType != processing-worker-3
          - node.labels.io.ethvm.nodeType != store
      replicas: 1
      restart_policy:
        condition: on-failure

  kafka-streams:
    image: enkryptio/kafka-streams:0.3.0
    depends_on:
      - kafka
      - zookeeper
      - kafka-schema-registry
    networks:
      - backend
    volumes:
      - kafka-streams-volume:/var/lib/kafka-streams
    environment:
      KAFKA_BOOTSTRAP_SERVERS: "kafka-1:9091, kafka-2:9091, kafka-3:9091"
      KAFKA_SCHEMA_REGISTRY_URL:
      KAFKA_STREAMS_STATE_DIR: /var/lib/kafka-streams
    deploy:
      resources:
        # limits:
        #   cpus: '3.0'
        #   memory: 4G
        # reservations:
        #   cpus: '2.0'
        #   memory: 2G
      placement:
        constraints:
          - node.role == worker
          - node.labels.io.ethvm.nodeType != processing-worker-1
          - node.labels.io.ethvm.nodeType != processing-worker-2
          - node.labels.io.ethvm.nodeType != processing-worker-3
          - node.labels.io.ethvm.nodeType != store
      replicas: 0
      restart_policy:
        condition: on-failure

  kafka-connect:
    image: enkryptio/kafka-connect:0.3.0
    restart: unless-stopped
    depends_on:
      - zookeeper-1
      - zookeeper-2
      - zookeeper-3
      - kafka-1
      - kafka-2
      - kafka-3
      - mongodb
      - kafka-schema-registry
    networks:
      - backend
    volumes:
      - kafka-connect-volume:/var/lib/kafka
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "kafka-1:9091, kafka-2:9091, kafka-3:9091"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: "ethvm-kafka-connect"
      CONNECT_STATUS_STORAGE_TOPIC: "ethvm-storage-topic"
      CONNECT_CONFIG_STORAGE_TOPIC: "ethvm-storage-config"
      CONNECT_OFFSET_STORAGE_TOPIC: "ethvm-storage-offsets"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_OFFSET_COMMIT_INTERVAL_MS: 1000
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://kafka-schema-registry:8081
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://kafka-schema-registry:8081
      CONNECT_KEY_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_VALUE_CONVERTER: "io.confluent.connect.avro.AvroConverter"
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_LOG4J_ROOT_LOGLEVEL: "INFO"
      CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR,io.enkrypt.kafka.connect=INFO,org.web3j.protocol.websocket.WebSocketService=WARN"
      KAFKA_JMX_PORT: 9588
      KAFKA_HEAP_OPTS: "-Xms8G -Xmx10G -server"
      CONNECT_KAFKA_HEAP_OPTS: "-Xms8G -Xmx10G -server"
      CONNECT_MAX_REQUEST_SIZE: 52428800
      CONNECT_PRODUCER_MAX_REQUEST_SIZE: 52428800
      CONNECT_CONSUMER_MAX_PARTITION_FETCH_BYTES: 52428800
      KAFKA_MAX_REQUEST_SIZE: 52428800
      KAFKA_PRODUCER_MAX_REQUEST_SIZE: 52428800
      KAFKA_CONSUMER_MAX_PARTITION_FETCH_BYTES: 52428800
      CONNECT_PLUGIN_PATH: /usr/share/confluent-hub-components,/usr/share/enkryptio
    deploy:
      # resources:
      #   limits:
      #     cpus: '4.0'
      #     memory: G
      #   reservations:
      #     cpus: '3.0'
      #     memory: 10G
      placement:
        constraints:
          - node.role == worker
          - node.labels.io.ethvm.nodeType != processing-worker-1
          - node.labels.io.ethvm.nodeType != processing-worker-2
          - node.labels.io.ethvm.nodeType != processing-worker-3
          - node.labels.io.ethvm.nodeType != store
      replicas: 1
      restart_policy:
        condition: on-failure

  # ///////////////////////////////////////////////////////
  # Etherum Client
  # ///////////////////////////////////////////////////////

  parity:
    image: parity/parity:v2.1.11
    networks:
      - backend
    volumes:
      - parity-volume:/home/parity/.local/share/io.parity.ethereum/
    ports:
      - 30303:30303
      - 30301:30301
    command: --chain ropsten --cache-size=4096 --tracing on --pruning archive --no-warp --bootnodes "enode://7778cd2215a3ff83c2191ca2cc303d9be1833e00ff5b945d94b4f256e0435ccda1f6abcb404bdd9862c5c255678a549b427b445991bc4f24fabb8229cbd33673@3.91.179.124:30303,enode://e5bb001edeb2342770bc03d00d89d04ba4536eb6ed82226eeb2206d8a3d511dd1c9cff0c9a09fc6cc0674424ec79a562d409258e9d04e2dc415bc0741bcaa44a@40.115.33.57:30305,enode://1447e19cb570a4d4dcaee418d410a5a242a26e9697a8d40180916576c0b0120b59af1759e4234e1e1f110accfe53566523d8073af30af93bbd3ddc20062a3851@104.248.159.93:30303,enode://15ac307a470b411745a6f10544ed54c0a14ad640b21f04f523e736e732bf709d4e28c2f06526ecabc03eed226b6d9bee8e433883cd20ab6cbd114bab77a8775d@52.176.7.10:30303,enode://e05b76d5f90860f13e066f7d5ae313290276181c48af4d753117ea1fe909a33b295ab37ada51fc0c0a9c8ffe618a0968c4e26f21cdc8a21f184ed7960c347ace@35.224.90.32:30303,enode://f0fa95ee17a42b4cf5fee104c5ad61115ff63b2f720c027b1246188afc93a61ba1d67121ea9ebfa78678e796a562a5d949ed5265549f0a4936b7f3bd8d5d2706@35.170.55.156:30303,enode://6ab5116c43c713096b3b3667ea9a24105bcb860bc81ec36f6b271694b9174bb1721232d56cfd2d9a5aa17fbac552833dec24282cfeb43af6572ae6f9bc157bf4@139.99.122.139:30302,enode://378b35851a8e55ddc607c8dd0b3f809787dd23200fa12fe112966de8ef77bb132b42d7adfa313c293a06ce7e3473495216fcd18cda6ff4814655ae734069be94@51.255.77.89:30302,enode://f0fa95ee17a42b4cf5fee104c5ad61115ff63b2f720c027b1246188afc93a61ba1d67121ea9ebfa78678e796a562a5d949ed5265549f0a4936b7f3bd8d5d2706@35.170.55.156:30303,enode://6ab5116c43c713096b3b3667ea9a24105bcb860bc81ec36f6b271694b9174bb1721232d56cfd2d9a5aa17fbac552833dec24282cfeb43af6572ae6f9bc157bf4@139.99.122.139:30302,enode://378b35851a8e55ddc607c8dd0b3f809787dd23200fa12fe112966de8ef77bb132b42d7adfa313c293a06ce7e3473495216fcd18cda6ff4814655ae734069be94@51.255.77.89:30302,enode://afc1345642b182b8455e89c2913d7393f513e31710c8175eaf288084873dbbce7b3c72cf5729b6e8e941767e7137528eb1406aad6b8b050d8f94dc1b221c2d35@172.31.34.252:30303" --ws-interface 0.0.0.0 --no-ipc
    deploy:
      placement:
        constraints:
          - node.role == worker
          - node.labels.io.ethvm.nodeType != processing-worker-1
          - node.labels.io.ethvm.nodeType != processing-worker-2
          - node.labels.io.ethvm.nodeType != processing-worker-3
          - node.labels.io.ethvm.nodeType != store
      replicas: 1
      restart_policy:
        condition: on-failure

  # ///////////////////////////////////////////////////////
  # Cluster Monitoring
  # ///////////////////////////////////////////////////////

  lenses:
    image: landoop/lenses:2.1
    networks:
      - backend
    ports:
      - 9991:9991
      - 9102:9102
    environment:
      LICENSE_URL: "${LENSES_LICENSE_URL}"
      LENSES_PORT: 9991
      LENSES_KAFKA_BROKERS: "PLAINTEXT://kafka-1:9091"
      LENSES_ZOOKEEPER_HOSTS: |
        [ { url:"zookeeper:2181", jmx:"zookeeper:9585" } ]
      LENSES_SCHEMA_REGISTRY_URLS: |
        [ { url:"http://kafka-schema-registry:8081", jmx:"kafka-schema-registry:9587" } ]
      LENSES_CONNECT_CLUSTERS: |
        [
          {
            name:"ethvm",
            urls: [ {url:"http://kafka-connect:8083", jmx:"kafka-connect:9588"} ],
            statuses:"ethvm-storage-topic",
            configs:"ethvm-storage-config",
            offsets:"ethvm-storage-offsets"
          }
        ]
      LENSES_SECURITY_MODE: BASIC
      LENSES_SECURITY_GROUPS: |
        [
          { "name": "adminGroup", "roles": ["Admin"] },
          { "name": "readGroup",  "roles": ["Read"] }
        ]
      LENSES_SECURITY_USERS: |
        [
          { "username": "admin", "password": "admin", "displayname": "Lenses Admin", "groups": ["adminGroup"] },
          { "username": "read", "password": "read", "displayname": "Read Only", "groups": ["readGroup"] }
        ]
    deploy:
      placement:
        constraints:
          - node.role == worker
          - node.labels.io.ethvm.nodeType != processing-worker-1
          - node.labels.io.ethvm.nodeType != processing-worker-2
          - node.labels.io.ethvm.nodeType != processing-worker-3
          - node.labels.io.ethvm.nodeType != store
      replicas: 1
      restart_policy:
        condition: on-failure
